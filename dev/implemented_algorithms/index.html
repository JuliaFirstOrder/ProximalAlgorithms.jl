<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Problem types and algorithms · ProximalAlgorithms.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">ProximalAlgorithms.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">User guide</span><ul><li><a class="tocitem" href="../getting_started/">Getting started</a></li><li class="is-active"><a class="tocitem" href>Problem types and algorithms</a><ul class="internal"><li><a class="tocitem" href="#two_terms_splitting"><span>Two-terms: <span>$f + g$</span></span></a></li><li><a class="tocitem" href="#three_terms_splitting"><span>Three-terms: <span>$f + g + h$</span></span></a></li><li><a class="tocitem" href="#primal_dual_splitting"><span>Primal-dual: <span>$f + g + h \circ L$</span></span></a></li></ul></li><li><a class="tocitem" href="../custom_objectives/">Custom objective terms</a></li></ul></li><li><a class="tocitem" href="../bibliography/">Bibliography</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">User guide</a></li><li class="is-active"><a href>Problem types and algorithms</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Problem types and algorithms</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/master/docs/src/implemented_algorithms.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="problems_algorithms"><a class="docs-heading-anchor" href="#problems_algorithms">Problem types and algorithms</a><a id="problems_algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#problems_algorithms" title="Permalink"></a></h1><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This page is under construction, and may be incomplete.</p></div></div><p>Depending on the structure a problem can be reduced to, different types of algorithms will apply. The major distinctions are in the number of objective terms, whether any of them is differentiable, whether they are composed with some linear mapping (which in general complicates evaluating the proximal mapping).</p><h2 id="two_terms_splitting"><a class="docs-heading-anchor" href="#two_terms_splitting">Two-terms: <span>$f + g$</span></a><a id="two_terms_splitting-1"></a><a class="docs-heading-anchor-permalink" href="#two_terms_splitting" title="Permalink"></a></h2><p>This is the most popular model, by far the most thoroughly studied, and an abundance of algorithms exist to solve problems in this form.</p><table><tr><th style="text-align: right">Algorithm</th><th style="text-align: right">Assumptions</th><th style="text-align: right">Oracle</th><th style="text-align: right">Implementation</th><th style="text-align: right">References</th></tr><tr><td style="text-align: right">Forward-backward</td><td style="text-align: right"><span>$f$</span> smooth</td><td style="text-align: right"><span>$\nabla f$</span>, <span>$\operatorname{prox}_{\gamma g}$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.ForwardBackwardIteration"><code>ForwardBackwardIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Lions1979">Pierre-Louis Lions, Bertrand Mercier (1979)</a></td></tr><tr><td style="text-align: right">Douglas-Rachford</td><td style="text-align: right"></td><td style="text-align: right"><span>$\operatorname{prox}_{\gamma f}$</span>, <span>$\operatorname{prox}_{\gamma g}$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.DouglasRachfordIteration"><code>DouglasRachfordIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Eckstein1992">Jonathan Eckstein, Dimitri P Bertsekas (1992)</a></td></tr><tr><td style="text-align: right">Fast forward-backward</td><td style="text-align: right"><span>$f$</span> convex, smooth, <span>$g$</span> convex</td><td style="text-align: right"><span>$\nabla f$</span>, <span>$\operatorname{prox}_{\gamma g}$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.FastForwardBackwardIteration"><code>FastForwardBackwardIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Tseng2008">Paul Tseng (2008)</a>, <a href="../bibliography/#Beck2009">Amir Beck, Marc Teboulle (2009)</a></td></tr><tr><td style="text-align: right">PANOC</td><td style="text-align: right"><span>$f$</span> smooth</td><td style="text-align: right"><span>$\nabla f$</span>, <span>$\operatorname{prox}_{\gamma g}$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.PANOCIteration"><code>PANOCIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Stella2017">Lorenzo Stella, Andreas Themelis, Pantelis Sopasakis, Panagiotis Patrinos (2017)</a></td></tr><tr><td style="text-align: right">ZeroFPR</td><td style="text-align: right"><span>$f$</span> smooth</td><td style="text-align: right"><span>$\nabla f$</span>, <span>$\operatorname{prox}_{\gamma g}$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.ZeroFPRIteration"><code>ZeroFPRIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Themelis2018">Andreas Themelis, Lorenzo Stella, Panagiotis Patrinos (2018)</a></td></tr><tr><td style="text-align: right">Douglas-Rachford line-search</td><td style="text-align: right"><span>$f$</span> smooth</td><td style="text-align: right"><span>$\operatorname{prox}_{\gamma f}$</span>, <span>$\operatorname{prox}_{\gamma g}$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.DRLSIteration"><code>DRLSIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Themelis2020">Andreas Themelis, Lorenzo Stella, Panagiotis Patrinos (2020)</a></td></tr><tr><td style="text-align: right">PANOC+</td><td style="text-align: right"><span>$f$</span> locally smooth</td><td style="text-align: right"><span>$\nabla f$</span>, <span>$\operatorname{prox}_{\gamma g}$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.PANOCplusIteration"><code>PANOCplusIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#DeMarchi2021">Alberto De Marchi, Andreas Themelis (2021)</a></td></tr></table><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.ForwardBackwardIteration" href="#ProximalAlgorithms.ForwardBackwardIteration"><code>ProximalAlgorithms.ForwardBackwardIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ForwardBackwardIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the forward-backward splitting algorithm (see [1]) for solving optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(x) + g(x),</code></pre><p>where <code>f</code> is smooth.</p><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial point.</li><li><code>f=Zero()</code>: smooth objective term.</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>Lf=nothing</code>: Lipschitz constant of the gradient of <code>f</code>.</li><li><code>gamma=nothing</code>: stepsize to use, defaults to <code>1/Lf</code> if not set (but <code>Lf</code> is).</li><li><code>adaptive=false</code>: forces the method stepsize to be adaptively adjusted.</li><li><code>minimum_gamma=1e-7</code>: lower bound to <code>gamma</code> in case <code>adaptive == true</code>.</li></ul><p><strong>References</strong></p><ol><li>Lions, Mercier, “Splitting algorithms for the sum of two nonlinear operators,” SIAM Journal on Numerical Analysis, vol. 16, pp. 964–979 (1979).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/forward_backward.jl#L10-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.DouglasRachfordIteration" href="#ProximalAlgorithms.DouglasRachfordIteration"><code>ProximalAlgorithms.DouglasRachfordIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DouglasRachfordIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the Douglas-Rachford splitting algorithm (see [1]) for solving convex optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(x) + g(x).</code></pre><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial point.</li><li><code>f=Zero()</code>: proximable objective term.</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>gamma</code>: stepsize to use.</li></ul><p><strong>References</strong></p><ol><li>Eckstein, Bertsekas, &quot;On the Douglas-Rachford Splitting Method and the Proximal Point Algorithm for Maximal Monotone Operators&quot;, Mathematical Programming, vol. 55, no. 1, pp. 293-318 (1989).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/douglas_rachford.jl#L11-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.FastForwardBackwardIteration" href="#ProximalAlgorithms.FastForwardBackwardIteration"><code>ProximalAlgorithms.FastForwardBackwardIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FastForwardBackwardIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the accelerated forward-backward splitting algorithm (see [1, 2]) for solving optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(x) + g(x),</code></pre><p>where <code>f</code> is smooth.</p><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial point.</li><li><code>f=Zero()</code>: smooth objective term.</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>Lf=nothing</code>: Lipschitz constant of the gradient of <code>f</code>.</li><li><code>gamma=nothing</code>: stepsize to use, defaults to <code>1/Lf</code> if not set (but <code>Lf</code> is).</li><li><code>adaptive=false</code>: forces the method stepsize to be adaptively adjusted.</li><li><code>minimum_gamma=1e-7</code>: lower bound to <code>gamma</code> in case <code>adaptive == true</code>.</li></ul><p><strong>References</strong></p><ol><li>Tseng, &quot;On Accelerated Proximal Gradient Methods for Convex-Concave Optimization&quot; (2008).</li><li>Beck, Teboulle, &quot;A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems&quot;, SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183-202 (2009).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/fast_forward_backward.jl#L14-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.PANOCIteration" href="#ProximalAlgorithms.PANOCIteration"><code>ProximalAlgorithms.PANOCIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PANOCIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the PANOC algorithm (see [1]) for solving optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(Ax) + g(x),</code></pre><p>where <code>f</code> is smooth and <code>A</code> is a linear mapping (for example, a matrix).</p><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial point.</li><li><code>f=Zero()</code>: smooth objective term.</li><li><code>A=I</code>: linear operator (e.g. a matrix).</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>Lf=nothing</code>: Lipschitz constant of the gradient of x ↦ f(Ax).</li><li><code>gamma=nothing</code>: stepsize to use, defaults to <code>1/Lf</code> if not set (but <code>Lf</code> is).</li><li><code>adaptive=false</code>: forces the method stepsize to be adaptively adjusted.</li><li><code>minimum_gamma=1e-7</code>: lower bound to <code>gamma</code> in case <code>adaptive == true</code>.</li><li><code>max_backtracks=20</code>: maximum number of line-search backtracks.</li><li><code>directions=LBFGS(5)</code>: strategy to use to compute line-search directions.</li></ul><p><strong>References</strong></p><ol><li>Stella, Themelis, Sopasakis, Patrinos, &quot;A simple and efficient algorithm for nonlinear model predictive control&quot;, 56th IEEE Conference on Decision and Control (2017).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/panoc.jl#L11-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.ZeroFPRIteration" href="#ProximalAlgorithms.ZeroFPRIteration"><code>ProximalAlgorithms.ZeroFPRIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ZeroFPRIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the ZeroFPR algorithm (see [1]) for solving optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(Ax) + g(x),</code></pre><p>where <code>f</code> is smooth and <code>A</code> is a linear mapping (for example, a matrix).</p><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial point.</li><li><code>f=Zero()</code>: smooth objective term.</li><li><code>A=I</code>: linear operator (e.g. a matrix).</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>Lf=nothing</code>: Lipschitz constant of the gradient of x ↦ f(Ax).</li><li><code>gamma=nothing</code>: stepsize to use, defaults to <code>1/Lf</code> if not set (but <code>Lf</code> is).</li><li><code>adaptive=false</code>: forces the method stepsize to be adaptively adjusted.</li><li><code>minimum_gamma=1e-7</code>: lower bound to <code>gamma</code> in case <code>adaptive == true</code>.</li><li><code>max_backtracks=20</code>: maximum number of line-search backtracks.</li><li><code>directions=LBFGS(5)</code>: strategy to use to compute line-search directions.</li></ul><p><strong>References</strong></p><ol><li>Themelis, Stella, Patrinos, &quot;Forward-backward envelope for the sum of two nonconvex functions: Further properties and nonmonotone line-search algorithms&quot;, SIAM Journal on Optimization, vol. 28, no. 3, pp. 2274–2303 (2018).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/zerofpr.jl#L12-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.DRLSIteration" href="#ProximalAlgorithms.DRLSIteration"><code>ProximalAlgorithms.DRLSIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DRLSIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the Douglas-Rachford line-search algorithm (see [1]) for solving optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(x) + g(x),</code></pre><p>where <code>f</code> is smooth.</p><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial point.</li><li><code>f=Zero()</code>: smooth objective term.</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>muf=nothing</code>: convexity modulus of f.</li><li><code>Lf=nothing</code>: Lipschitz constant of the gradient of f.</li><li><code>gamma</code>: stepsize to use, chosen appropriately based on Lf and muf by defaults.</li><li><code>max_backtracks=20</code>: maximum number of line-search backtracks.</li><li><code>directions=LBFGS(5)</code>: strategy to use to compute line-search directions.</li></ul><p><strong>References</strong></p><ol><li>Themelis, Stella, Patrinos, &quot;Douglas-Rachford splitting and ADMM for nonconvex optimization: Accelerated and Newton-type linesearch algorithms&quot;, arXiv:2005.10230, 2020.</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/drls.jl#L30-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.PANOCplusIteration" href="#ProximalAlgorithms.PANOCplusIteration"><code>ProximalAlgorithms.PANOCplusIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PANOCplusIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the PANOCplus algorithm (see [1]) for solving optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(Ax) + g(x),</code></pre><p>where <code>f</code> is locally smooth and <code>A</code> is a linear mapping (for example, a matrix).</p><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial point.</li><li><code>f=Zero()</code>: smooth objective term.</li><li><code>A=I</code>: linear operator (e.g. a matrix).</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>Lf=nothing</code>: Lipschitz constant of the gradient of x ↦ f(Ax).</li><li><code>gamma=nothing</code>: stepsize to use, defaults to <code>1/Lf</code> if not set (but <code>Lf</code> is).</li><li><code>adaptive=false</code>: forces the method stepsize to be adaptively adjusted.</li><li><code>minimum_gamma=1e-7</code>: lower bound to <code>gamma</code> in case <code>adaptive == true</code>.</li><li><code>max_backtracks=20</code>: maximum number of line-search backtracks.</li><li><code>directions=LBFGS(5)</code>: strategy to use to compute line-search directions.</li></ul><p><strong>References</strong></p><ol><li>De Marchi, Themelis, &quot;Proximal gradient algorithms under local Lipschitz gradient continuity: a convergence and robustness analysis of PANOC&quot;, arXiv:2112.13000 (2021).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/panocplus.jl#L11-L35">source</a></section></article><h2 id="three_terms_splitting"><a class="docs-heading-anchor" href="#three_terms_splitting">Three-terms: <span>$f + g + h$</span></a><a id="three_terms_splitting-1"></a><a class="docs-heading-anchor-permalink" href="#three_terms_splitting" title="Permalink"></a></h2><p>When more than one non-differentiable term is there in the objective, algorithms from the <a href="#two_terms_splitting">previous section</a> do not <em>in general</em> apply out of the box, since <span>$\operatorname{prox}_{\gamma (f + g)}$</span> does not have a closed form unless in particular cases. Therefore, ad-hoc iteration schemese have been studied.</p><table><tr><th style="text-align: right">Algorithm</th><th style="text-align: right">Assumptions</th><th style="text-align: right">Oracle</th><th style="text-align: right">Implementation</th><th style="text-align: right">References</th></tr><tr><td style="text-align: right">Davis-Yin</td><td style="text-align: right"><span>$f, g$</span> convex, <span>$h$</span> convex and smooth</td><td style="text-align: right"><span>$\operatorname{prox}_{\gamma f}$</span>, <span>$\operatorname{prox}_{\gamma g}$</span>, <span>$\nabla h$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.DavisYinIteration"><code>DavisYinIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Davis2017">Damek Davis, Wotao Yin (2017)</a></td></tr></table><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.DavisYinIteration" href="#ProximalAlgorithms.DavisYinIteration"><code>ProximalAlgorithms.DavisYinIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DavisYinIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the Davis-Yin splitting algorithm (see [1]) for solving convex optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(x) + g(x) + h(x),</code></pre><p>where <code>h</code> is smooth.</p><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial point.</li><li><code>f=Zero()</code>: proximable objective term.</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>h=Zero()</code>: smooth objective term.</li><li><code>Lh=nothing</code>: Lipschitz constant of the gradient of h.</li><li><code>gamma=nothing</code>: stepsize to use, defaults to <code>1/Lh</code> if not set (but <code>Lh</code> is).</li></ul><p><strong>References</strong></p><ol><li>Davis, Yin. &quot;A Three-Operator Splitting Scheme and its Optimization Applications&quot;, Set-Valued and Variational Analysis, vol. 25, no. 4, pp. 829–858 (2017).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/davis_yin.jl#L11-L31">source</a></section></article><h2 id="primal_dual_splitting"><a class="docs-heading-anchor" href="#primal_dual_splitting">Primal-dual: <span>$f + g + h \circ L$</span></a><a id="primal_dual_splitting-1"></a><a class="docs-heading-anchor-permalink" href="#primal_dual_splitting" title="Permalink"></a></h2><p>When a function <span>$h$</span> is composed with a linear operator <span>$L$</span>, the proximal operator of <span>$h \circ L$</span> does not have a closed form in general. For this reason, specific algorithms by the name of &quot;primal-dual&quot; splitting schemes are often applied to this model.</p><table><tr><th style="text-align: right">Algorithm</th><th style="text-align: right">Assumptions</th><th style="text-align: right">Oracle</th><th style="text-align: right">Implementation</th><th style="text-align: right">References</th></tr><tr><td style="text-align: right">Vu-Condat</td><td style="text-align: right"><span>$f$</span> convex and smooth, <span>$g, h$</span> convex, <span>$L$</span> linear operator</td><td style="text-align: right"><span>$\nabla f$</span>, <span>$\operatorname{prox}_{\gamma g}$</span>, <span>$\operatorname{prox}_{\gamma h}$</span>, <span>$L$</span>, <span>$L^*$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.VuCondatIteration"><code>VuCodatIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Vu2013">Bằng Công Vũ (2013)</a>, <a href="../bibliography/#Condat2013">Laurent Condat (2013)</a></td></tr><tr><td style="text-align: right">AFBA</td><td style="text-align: right"><span>$f$</span> convex and smooth, <span>$g, h$</span> convex, <span>$L$</span> linear operator</td><td style="text-align: right"><span>$\nabla f$</span>, <span>$\operatorname{prox}_{\gamma g}$</span>, <span>$\operatorname{prox}_{\gamma h}$</span>, <span>$L$</span>, <span>$L^*$</span></td><td style="text-align: right"><a href="#ProximalAlgorithms.AFBAIteration"><code>AFBAIteration</code></a></td><td style="text-align: right"><a href="../bibliography/#Latafat2017">Puya Latafat, Panagiotis Patrinos (2017)</a></td></tr></table><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.AFBAIteration" href="#ProximalAlgorithms.AFBAIteration"><code>ProximalAlgorithms.AFBAIteration</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AFBAIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the asymmetric forward-backward-adjoint algorithm (AFBA, see [1]) for solving convex optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(x) + g(x) + (h □ l)(L x),</code></pre><p>where <code>f</code> is smooth, <code>g</code> and <code>h</code> are possibly nonsmooth and <code>l</code> is strongly convex. Symbol <code>□</code> denotes the infimal convolution, and <code>L</code> is a linear mapping.</p><p>Points <code>x0</code> and <code>y0</code> are the initial primal and dual iterates, respectively. If unspecified, functions <code>f</code>, <code>g</code>, and <code>h</code> default to the identically zero function, <code>l</code> defaults to the indicator of the set <code>{0}</code>, and <code>L</code> defaults to the identity. Important keyword arguments, in case <code>f</code> and <code>l</code> are set, are the Lipschitz constants <code>beta_f</code> and <code>beta_l</code> (see below).</p><p>The iterator implements Algorithm 3 of [1] with constant stepsize (α_n=λ) for several prominant special cases:</p><ol><li>θ = 2          ==&gt;   Corresponds to the Vu-Condat Algorithm [2,3].</li><li>θ = 1, μ=1</li><li>θ = 0, μ=1</li><li>θ ∈ [0,∞), μ=0</li></ol><p>See [2, Section 5.2] and [1, Figure 1] for stepsize conditions, special cases, and relation to other algorithms.</p><p><strong>Arguments</strong></p><ul><li><code>x0</code>: initial primal point.</li><li><code>y0</code>: initial dual point.</li><li><code>f=Zero()</code>: smooth objective term.</li><li><code>g=Zero()</code>: proximable objective term.</li><li><code>h=Zero()</code>: proximable objective term.</li><li><code>l=IndZero()</code>: strongly convex function.</li><li><code>L=I</code>: linear operator (e.g. a matrix).</li><li><code>beta_f=0</code>: Lipschitz constant of the gradient of <code>f</code>.</li><li><code>beta_l=0</code>: Lipschitz constant of the gradient of <code>l</code> conjugate.</li><li><code>theta=1</code>: nonnegative algorithm parameter.</li><li><code>mu=1</code>: algorithm parameter in the range [0,1].</li><li><code>gamma1</code>: primal stepsize (see [1] for the default choice).</li><li><code>gamma2</code>: dual stepsize (see [1] for the default choice).</li></ul><p><strong>References</strong></p><ol><li>Latafat, Patrinos, &quot;Asymmetric forward–backward–adjoint splitting for solving monotone inclusions involving three operators&quot;, Computational Optimization and Applications, vol. 68, no. 1, pp. 57-93 (2017).</li><li>Latafat, Patrinos, &quot;Primal-dual proximal algorithms for structured convex optimization : a unifying framework&quot;, In Large-Scale and Distributed Optimization, Giselsson and Rantzer, Eds. Springer International Publishing, pp. 97–120 ( 2018).</li><li>Condat, &quot;A primal–dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms&quot;, Journal of Optimization Theory and Applications, vol. 158, no. 2, pp 460-479 (2013).</li><li>Vũ, &quot;A splitting algorithm for dual monotone inclusions involving cocoercive operators&quot;, Advances in Computational Mathematics, vol. 38, no. 3, pp. 667-681 (2013).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/primal_dual.jl#L29-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.VuCondatIteration" href="#ProximalAlgorithms.VuCondatIteration"><code>ProximalAlgorithms.VuCondatIteration</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">VuCondatIteration(; &lt;keyword-arguments&gt;)</code></pre><p>Instantiate the Vũ-Condat primal-dual algorithm (see [1, 2]) for solving convex optimization problems of the form</p><pre><code class="nohighlight hljs">minimize f(x) + g(x) + (h □ l)(L x),</code></pre><p>where <code>f</code> is smooth, <code>g</code> and <code>h</code> are possibly nonsmooth and <code>l</code> is strongly convex. Symbol <code>□</code> denotes the infimal convolution, and <code>L</code> is a linear mapping.</p><p>For the arguments see <a href="#ProximalAlgorithms.AFBAIteration"><code>AFBAIteration</code></a>.</p><p><strong>References</strong></p><ol><li>Condat, &quot;A primal–dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms&quot;, Journal of Optimization Theory and Applications, vol. 158, no. 2, pp 460-479 (2013).</li><li>Vũ, &quot;A splitting algorithm for dual monotone inclusions involving cocoercive operators&quot;, Advances in Computational Mathematics, vol. 38, no. 3, pp. 667-681 (2013).</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/9f6e5803b98ae988a3d1ab21319b46bf5106a127/src/algorithms/primal_dual.jl#L99-L115">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../getting_started/">« Getting started</a><a class="docs-footer-nextpage" href="../custom_objectives/">Custom objective terms »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Thursday 13 January 2022 10:16">Thursday 13 January 2022</span>. Using Julia version 1.6.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
