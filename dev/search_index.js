var documenterSearchIndex = {"docs":
[{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"EditURL = \"https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/master/docs/src/examples/sparse_linear_regression.jl\"","category":"page"},{"location":"examples/sparse_linear_regression/#sparse_linreg","page":"Sparse linear regression","title":"Sparse linear regression","text":"","category":"section"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"Let's look at a least squares regression problem with L1 regularization: we will use the \"diabetes dataset\" (see here), so let's start by loading the data.","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"using HTTP\n\nsplitlines(s) = split(s, \"\\n\")\nsplitfields(s) = split(s, \"\\t\")\nparsefloat64(s) = parse(Float64, s)\n\nfunction load_diabetes_dataset()\n    res = HTTP.request(\"GET\", \"https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt\")\n    lines = res.body |> String |> strip |> splitlines\n    return hcat((line |> splitfields .|> parsefloat64 for line in lines[2:end])...)'\nend\n\ndata = load_diabetes_dataset()\n\ntraining_input = data[1:end-100, 1:end-1]\ntraining_label = data[1:end-100, end]\n\ntest_input = data[end-99:end, 1:end-1]\ntest_label = data[end-99:end, end]\n\nn_training, n_features = size(training_input)","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"Now we can set up the optimization problem we want to solve: we will minimize the mean squared error for a linear model, appropriately scaled so that the features in the training data are normally distributed (\"standardization\", this is known to help the optimization process).","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"After some simple manipulation, this standardized linear model can be implemented as follows:","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"using LinearAlgebra\nusing Statistics\n\ninput_loc = mean(training_input, dims=1) |> vec\ninput_scale = std(training_input, dims=1) |> vec\n\nlinear_model(wb, input) = input * wb[1:end-1] .+ wb[end]\n\nfunction standardized_linear_model(wb, input)\n    w_scaled = wb[1:end-1] ./ input_scale\n    wb_scaled = vcat(w_scaled, wb[end] - dot(w_scaled, input_loc))\n    return linear_model(wb_scaled, input)\nend","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"The loss term in the cost is then the following. Note that this is a regular Julia function: since the algorithm we will apply requires its gradient, automatic differentiation will do the work for us.","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"mean_squared_error(label, output) = mean((output .- label) .^ 2) / 2\n\ntraining_loss(wb) = mean_squared_error(training_label, standardized_linear_model(wb, training_input))","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"As regularization we will use the L1 norm, implemented in ProximalOperators:","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"using ProximalOperators\n\nreg = ProximalOperators.NormL1(1)","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"We want to minimize the sum of training_loss and reg, and for this task we can use FastForwardBackward, which implements the fast proximal gradient method (also known as fast forward-backward splitting, or FISTA). Therefore we construct the algorithm, then apply it to our problem by providing a starting point, and the objective terms f=training_loss (smooth) and g=reg (non smooth).","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"using ProximalAlgorithms\n\nffb = ProximalAlgorithms.FastForwardBackward()\nsolution, iterations = ffb(x0=zeros(n_features + 1), f=training_loss, g=reg)","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"We can now check how well the trained model performs on the test portion of our data.","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"test_output = standardized_linear_model(solution, test_input)\nmean_squared_error(test_label, test_output)","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"","category":"page"},{"location":"examples/sparse_linear_regression/","page":"Sparse linear regression","title":"Sparse linear regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"EditURL = \"https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/master/docs/src/guide/getting_started.jl\"","category":"page"},{"location":"guide/getting_started/#Getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"The methods implemented in ProximalAlgorithms are commonly referred to as (you've guessed it) proximal algorithms, in that they rely on the proximal operator (or mapping) to deal with non-differentiable terms in the objective. Loosely speaking, the algorithms in this package can be used to solve problems of the form","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"operatorname*minimize_x sum_i=1^N f_i(x)","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"where N depends on the specific algorithm, together with specific assumptions on the terms f_i (like smoothness, convexity, strong convexity). The problem above is solved by iteratively accessing specific first order information on the terms f_i, like their gradient nabla f_i or their proximal mapping operatornameprox_f_i:","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"mathrmprox_gamma f_i(x) = argmin_z left f_i(z) + tfrac12gammaz-x^2 right","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"The literature on proximal operators and algorithms is vast: for an overview, one can refer to Neal Parikh (2014), Amir Beck (2017).","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"To evaluate these first-order primitives, in ProximalAlgorithms:","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"nabla f_i falls back to using automatic differentiation (as provided by Zygote).\noperatornameprox_f_i relies on the intereface of ProximalOperators.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Both of the above can be implemented for custom function types, as documented here.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"note: Note\nEach of the implemented algorithms assumes a different structure of the objective to be optimized (e.g., a specific number N of terms), with specific assumptions (e.g., smoothness, convexity). Furthermore, multiple algorithms can often be applied to the same problem (possibly through appropriate reformulation) and are expected to perform differently; sometimes, even the same algorithm can be applied in multiple ways to the same problem, by grouping (splitting) the terms in different ways.Because of these reasons, ProximalAlgorithms does not offer a modeling language to automagically minimize any objective: rather, the user is expected to formulate their problem by providing the right objective terms to the algorithm of choice. Please refer to the this section of the manual for information on what terms can be provided and under which assumptions.","category":"page"},{"location":"guide/getting_started/#algorithm_interface","page":"Getting started","title":"Interface to algorithms","text":"","category":"section"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"At a high level, using algorithms from ProximalAlgorithms amounts to the following.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Instantiate the algorithm, with options like the termination tolerance, verbosity level, or other algorithm-specific parameters.\nCall the algorithm on the problem description: this amounts to the initial point, the objective terms, and possibly additional required information (e.g. Lipschitz constants).","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"See here for the list of available algorithm constructors, for different types of problems. In general however, algorithms are instances of the IterativeAlgorithm type.","category":"page"},{"location":"guide/getting_started/#box_qp","page":"Getting started","title":"Example: box constrained quadratic","text":"","category":"section"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"As a simple example, consider the minimization of a 2D quadratic function subject to box constraints, which we will solve using the fast proximal gradient method (also known as fast forward-backward splitting):","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"using LinearAlgebra\nusing ProximalOperators\nusing ProximalAlgorithms\n\nquadratic_cost(x) = dot([3.4 1.2; 1.2 89.1] * x, x) / 2 + dot([-2.3, 99.9], x)\nbox_indicator = ProximalOperators.IndBox(0, 1)\n\nffb = ProximalAlgorithms.FastForwardBackward(maxit=1000, tol=1e-5, verbose=true)","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Here, we defined the cost function quadratic_cost, and the constraint indicator box_indicator. Then we set up the optimization algorithm of choice, FastForwardBackward, with options for the maximum number of iterations, termination tolerance, verbosity. Finally, we run the algorithm by providing an initial point and the objective terms defining the problem:","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"solution, iterations = ffb(x0=ones(2), f=quadratic_cost, g=box_indicator)","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"We can verify the correctness of the solution by checking that the negative gradient is orthogonal to the constraints, pointing outwards:","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"-ProximalAlgorithms.gradient(quadratic_cost, solution)[1]","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Or by plotting the solution against the cost function and constraint:","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"using Plots\n\ncontour(-1:0.1:2, -1:0.1:2, (x,y) -> quadratic_cost([x, y]), fill=true, framestyle=:none, background=nothing)\nplot!(Shape([0, 1, 1, 0], [0, 0, 1, 1]), opacity=.5, label=\"feasible set\")\nscatter!([solution[1]], [solution[2]], color=:red, markershape=:star5, label=\"computed solution\")","category":"page"},{"location":"guide/getting_started/#iterator_interface","page":"Getting started","title":"Iterator interface","text":"","category":"section"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Under the hood, algorithms are implemented in the form of standard Julia iterators: constructing such iterator objects directly, and looping over them, allows for more fine-grained control over the termination condition, or what information from the iterations get logged.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Each iterator is constructed with the full problem description (objective terms and, if needed, additional information like Lipschitz constats) and algorithm options (usually step sizes, and any other parameter or option of the algorithm), and produces the sequence of states of the algorithm, so that one can do (almost) anything with it.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"note: Note\nIterators only implement the algorithm iteration logic, and not additional details like stopping criteria. As such, iterators usually yield an infinite sequence of states: when looping over them, be careful to properly guard the loop with a stopping criterion.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"warning: Warning\nTo save on allocations, most (if not all) algorithms re-use state objects when iterating, by updating the state in place instead of creating a new one. For this reason:one should not mutate the state object in any way, as this may corrupt the algorithm's logic;\none should not collect the sequence of states, since this will result in an array of identical objects.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Iterator types are named after the algorithm they implement, so the relationship should be obvious:","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"the ForwardBackward algorithm uses the ForwardBackwardIteration iterator type;\nthe FastForwardBackward algorithm uses the FastForwardBackwardIteration iterator type;\nthe DouglasRachford algorithm uses the DouglasRachfordIteration iterator type;","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"and so on.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Let's see what this means in terms of the previous example.","category":"page"},{"location":"guide/getting_started/#box_qp_cont","page":"Getting started","title":"Example: box constrained quadratic (cont)","text":"","category":"section"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"Let's solve the problem from the previous example by directly interacting with the underlying iterator: the FastForwardBackward algorithm internally uses a FastForwardBackwardIteration object.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"ffbiter = ProximalAlgorithms.FastForwardBackwardIteration(x0=ones(2), f=quadratic_cost, g=box_indicator)","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"We can now perform anything we want throughout the iteration, by just looping over the iterator: for example, we can store the sequence of iterates from the algorithm, to later plot them, and stop whenever two successive iterates are closer than a given tolerance.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"xs = []\nfor state in ffbiter\n    push!(xs, copy(state.x))\n    if length(xs) > 1 && norm(xs[end] - xs[end - 1]) / (1 + norm(xs[end])) <= 1e-5\n        break\n    end\nend\n\ncontour(-1:0.1:2, -1:0.1:2, (x,y) -> quadratic_cost([x, y]), fill=true, framestyle=:none, background=nothing)\nplot!(Shape([0, 1, 1, 0], [0, 0, 1, 1]), opacity=.5, label=\"feasible set\")\nplot!([x[1] for x in xs], [x[2] for x in xs], markershape=:circle, label=\"algorithm trajectory\")\nscatter!([solution[1]], [solution[2]], color=:red, markershape=:star5, label=\"computed solution\")","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"note: Note\nSince each algorithm iterator type has its own logic, it will also have its own dedicated state structure. Interacting with the state then requires being familiar with its structure, and with the nature of its attributes.","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"","category":"page"},{"location":"guide/getting_started/","page":"Getting started","title":"Getting started","text":"This page was generated using Literate.jl.","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"EditURL = \"https://github.com/JuliaFirstOrder/ProximalAlgorithms.jl/blob/master/docs/src/guide/custom_objectives.jl\"","category":"page"},{"location":"guide/custom_objectives/#custom_terms","page":"Custom objective terms","title":"Custom objective terms","text":"","category":"section"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"ProximalAlgorithms relies on the first-order primitives implemented in ProximalOperators: while a rich library of function types is provided there, one may need to formulate problems using custom objective terms. When that is the case, one only needs to implement the right first-order primitive, nabla f or operatornameprox_gamma f or both, for algorithms to be able to work with f.","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"Defining the proximal mapping for a custom function type requires adding a method for prox!.","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"To compute gradients, ProximalAlgorithms provides a fallback definition for gradient!, relying on Zygote to use automatic differentiation. Therefore, you can provide any (differentiable) Julia function wherever gradients need to be taken, and everything will work out of the box.","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"If however one would like to provide their own gradient implementation (e.g. for efficiency reasons), they can simply implement a method for gradient!.","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"ProximalAlgorithms.prox!(y, f, x, gamma)\nProximalAlgorithms.gradient!(g, f, x)","category":"page"},{"location":"guide/custom_objectives/#ProximalOperators.prox!-NTuple{4, Any}","page":"Custom objective terms","title":"ProximalOperators.prox!","text":"prox!(y, f, x, gamma)\n\nCompute the proximal mapping of f at x, with stepsize gamma, and store the result in y. Return the value of f at y.\n\n\n\n\n\n","category":"method"},{"location":"guide/custom_objectives/#ProximalOperators.gradient!-Tuple{Any, Any, Any}","page":"Custom objective terms","title":"ProximalOperators.gradient!","text":"gradient!(g, f, x)\n\nCompute the gradient of f at x, and stores it in y. Return the value of f at x.\n\n\n\n\n\n","category":"method"},{"location":"guide/custom_objectives/#Example:-constrained-Rosenbrock","page":"Custom objective terms","title":"Example: constrained Rosenbrock","text":"","category":"section"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"Let's try to minimize the celebrated Rosenbrock function, but constrained to the unit norm ball. The cost function is","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"rosenbrock2D(x) = 100 * (x[2] - x[1]^2)^2 + (1 - x[1])^2","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"To enforce the constraint, we define the indicator of the unit ball, together with its proximal mapping: this is simply projection onto the unit norm ball, so it is sufficient to normalize any given point that lies outside of the set.","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"using LinearAlgebra\nusing ProximalOperators\n\nstruct IndUnitBall <: ProximalOperators.ProximableFunction end\n\n(::IndUnitBall)(x) = norm(x) > 1 ? eltype(x)(Inf) : eltype(x)(0)\n\nfunction ProximalOperators.prox!(y, ::IndUnitBall, x, gamma)\n    if norm(x) > 1\n        y .= x ./ norm(x)\n    else\n        y .= x\n    end\n    return zero(eltype(x))\nend","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"We can now minimize the function, for which we will use PANOC, which is a Newton-type method:","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"using ProximalAlgorithms\n\npanoc = ProximalAlgorithms.PANOC()\nsolution, iterations = panoc(x0=-ones(2), f=rosenbrock2D, g=IndUnitBall())","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"Plotting the solution against the cost function contour and constraint, gives an idea of its correctness.","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"using Plots\n\ncontour(-2:0.1:2, -2:0.1:2, (x,y) -> rosenbrock2D([x, y]), fill=true, framestyle=:none, background=nothing)\nplot!(Shape(cos.(0:0.01:2*pi), sin.(0:0.01:2*pi)), opacity=.5, label=\"feasible set\")\nscatter!([solution[1]], [solution[2]], color=:red, markershape=:star5, label=\"computed solution\")","category":"page"},{"location":"guide/custom_objectives/#Example:-counting-operations","page":"Custom objective terms","title":"Example: counting operations","text":"","category":"section"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"It is often interesting to measure how many operations (gradient- or prox-evaluation) an algorithm is taking. In fact, in algorithms involving backtracking or some other line-search logic, the iteration count may not be entirely representative of the amount of operations are being performed; or maybe some specific implementations require additional operations to be performed when checking stopping conditions. All of this makes it difficult to quantify the exact iteration complexity.","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"We can achieve this by wrapping functions in a dedicated Counting type:","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"mutable struct Counting{T} <: ProximalOperators.ProximableFunction\n    f::T\n    gradient_count::Int\n    prox_count::Int\nend\n\nCounting(f::T) where T = Counting{T}(f, 0, 0)","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"Now we only need to intercept any call to gradient! and prox! and increase counters there:","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"function ProximalOperators.gradient!(y, f::Counting, x)\n    f.gradient_count += 1\n    return ProximalOperators.gradient!(y, f.f, x)\nend\n\nfunction ProximalOperators.prox!(y, f::Counting, x, gamma)\n    f.prox_count += 1\n    return ProximalOperators.prox!(y, f.f, x, gamma)\nend","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"We can run again the previous example, this time wrapping the objective terms within Counting:","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"f = Counting(rosenbrock2D)\ng = Counting(IndUnitBall())\n\nsolution, iterations = panoc(x0=-ones(2), f=f, g=g)","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"and check how many operations where actually performed:","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"println(f.gradient_count)\nprintln(g.prox_count)","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"","category":"page"},{"location":"guide/custom_objectives/","page":"Custom objective terms","title":"Custom objective terms","text":"This page was generated using Literate.jl.","category":"page"},{"location":"guide/implemented_algorithms/#problems_algorithms","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"","category":"section"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"warning: Warning\nThis page is under construction, and may be incomplete.","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"Depending on the structure a problem can be reduced to, different types of algorithms will apply. The major distinctions are in the number of objective terms, whether any of them is differentiable, whether they are composed with some linear mapping (which in general complicates evaluating the proximal mapping). Based on this we can split problems, and algorithms that apply to them, in three categories:","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"Two-terms: f + g\nThree-terms: f + g + h\nPrimal-dual: f + g + h circ L","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"In what follows, the list of available algorithms is given, with links to the documentation for their constructors and their underlying iterator type.","category":"page"},{"location":"guide/implemented_algorithms/#two_terms_splitting","page":"Problem types and algorithms","title":"Two-terms: f + g","text":"","category":"section"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"This is the most popular model, by far the most thoroughly studied, and an abundance of algorithms exist to solve problems in this form.","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"Algorithm Assumptions Oracle Implementation References\nForward-backward f smooth nabla f, operatornameprox_gamma g ForwardBackward Pierre-Louis Lions, Bertrand Mercier (1979)\nDouglas-Rachford  operatornameprox_gamma f, operatornameprox_gamma g DouglasRachford Jonathan Eckstein, Dimitri P Bertsekas (1992)\nFast forward-backward f convex, smooth, g convex nabla f, operatornameprox_gamma g FastForwardBackward Paul Tseng (2008), Amir Beck, Marc Teboulle (2009)\nPANOC f smooth nabla f, operatornameprox_gamma g PANOC Lorenzo Stella, Andreas Themelis, Pantelis Sopasakis, Panagiotis Patrinos (2017)\nZeroFPR f smooth nabla f, operatornameprox_gamma g ZeroFPR Andreas Themelis, Lorenzo Stella, Panagiotis Patrinos (2018)\nDouglas-Rachford line-search f smooth operatornameprox_gamma f, operatornameprox_gamma g DRLS Andreas Themelis, Lorenzo Stella, Panagiotis Patrinos (2020)\nPANOC+ f locally smooth nabla f, operatornameprox_gamma g PANOCplus Alberto De Marchi, Andreas Themelis (2021)","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"ProximalAlgorithms.ForwardBackward\nProximalAlgorithms.ForwardBackwardIteration\nProximalAlgorithms.DouglasRachford\nProximalAlgorithms.DouglasRachfordIteration\nProximalAlgorithms.FastForwardBackward\nProximalAlgorithms.FastForwardBackwardIteration\nProximalAlgorithms.PANOC\nProximalAlgorithms.PANOCIteration\nProximalAlgorithms.ZeroFPR\nProximalAlgorithms.ZeroFPRIteration\nProximalAlgorithms.DRLS\nProximalAlgorithms.DRLSIteration\nProximalAlgorithms.PANOCplus\nProximalAlgorithms.PANOCplusIteration","category":"page"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.ForwardBackward","page":"Problem types and algorithms","title":"ProximalAlgorithms.ForwardBackward","text":"ForwardBackward(; <keyword-arguments>)\n\nConstructs the forward-backward splitting algorithm [1].\n\nThis algorithm solves optimization problems of the form\n\nminimize f(x) + g(x),\n\nwhere f is smooth.\n\nThe returned object has type IterativeAlgorithm{ForwardBackwardIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: ForwardBackwardIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=10_000: maximum number of iteration\ntol::1e-8: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=100: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the ForwardBackwardIteration constructor upon call\n\nReferences\n\nLions, Mercier, “Splitting algorithms for the sum of two nonlinear operators,” SIAM Journal on Numerical Analysis, vol. 16, pp. 964–979 (1979).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.ForwardBackwardIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.ForwardBackwardIteration","text":"ForwardBackwardIteration(; <keyword-arguments>)\n\nIterator implementing the forward-backward splitting algorithm [1].\n\nThis iterator solves optimization problems of the form\n\nminimize f(x) + g(x),\n\nwhere f is smooth.\n\nSee also: ForwardBackward.\n\nArguments\n\nx0: initial point.\nf=Zero(): smooth objective term.\ng=Zero(): proximable objective term.\nLf=nothing: Lipschitz constant of the gradient of f.\ngamma=nothing: stepsize to use, defaults to 1/Lf if not set (but Lf is).\nadaptive=false: forces the method stepsize to be adaptively adjusted.\nminimum_gamma=1e-7: lower bound to gamma in case adaptive == true.\n\nReferences\n\nLions, Mercier, “Splitting algorithms for the sum of two nonlinear operators,” SIAM Journal on Numerical Analysis, vol. 16, pp. 964–979 (1979).\n\n\n\n\n\n","category":"type"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.DouglasRachford","page":"Problem types and algorithms","title":"ProximalAlgorithms.DouglasRachford","text":"DouglasRachford(; <keyword-arguments>)\n\nConstructs the Douglas-Rachford splitting algorithm [1].\n\nThis algorithm solves convex optimization problems of the form\n\nminimize f(x) + g(x).\n\nThe returned object has type IterativeAlgorithm{DouglasRachfordIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: DouglasRachfordIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=1_000: maximum number of iteration\ntol::1e-8: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=100: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the DouglasRachfordIteration constructor upon call\n\nReferences\n\nEckstein, Bertsekas, \"On the Douglas-Rachford Splitting Method and the Proximal Point Algorithm for Maximal Monotone Operators\", Mathematical Programming, vol. 55, no. 1, pp. 293-318 (1989).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.DouglasRachfordIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.DouglasRachfordIteration","text":"DouglasRachfordIteration(; <keyword-arguments>)\n\nIterator implementing the Douglas-Rachford splitting algorithm [1].\n\nThis iterator solves convex optimization problems of the form\n\nminimize f(x) + g(x).\n\nSee also: DouglasRachford.\n\nArguments\n\nx0: initial point.\nf=Zero(): proximable objective term.\ng=Zero(): proximable objective term.\ngamma: stepsize to use.\n\nReferences\n\nEckstein, Bertsekas, \"On the Douglas-Rachford Splitting Method and the Proximal Point Algorithm for Maximal Monotone Operators\", Mathematical Programming, vol. 55, no. 1, pp. 293-318 (1989).\n\n\n\n\n\n","category":"type"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.FastForwardBackward","page":"Problem types and algorithms","title":"ProximalAlgorithms.FastForwardBackward","text":"FastForwardBackward(; <keyword-arguments>)\n\nConstructs the accelerated forward-backward splitting algorithm [1, 2].\n\nThis algorithm solves convex optimization problems of the form\n\nminimize f(x) + g(x),\n\nwhere f is smooth.\n\nThe returned object has type IterativeAlgorithm{FastForwardBackwardIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: FastForwardBackwardIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=10_000: maximum number of iteration\ntol::1e-8: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=100: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the FastForwardBackwardIteration constructor upon call\n\nReferences\n\nTseng, \"On Accelerated Proximal Gradient Methods for Convex-Concave Optimization\" (2008).\nBeck, Teboulle, \"A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems\", SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183-202 (2009).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.FastForwardBackwardIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.FastForwardBackwardIteration","text":"FastForwardBackwardIteration(; <keyword-arguments>)\n\nIterator implementing the accelerated forward-backward splitting algorithm [1, 2].\n\nThis iterator solves convex optimization problems of the form\n\nminimize f(x) + g(x),\n\nwhere f is smooth.\n\nSee also: FastForwardBackward.\n\nArguments\n\nx0: initial point.\nf=Zero(): smooth objective term.\ng=Zero(): proximable objective term.\nmf=0: convexity modulus of f.\nLf=nothing: Lipschitz constant of the gradient of f.\ngamma=nothing: stepsize, defaults to 1/Lf if Lf is set, and nothing otherwise.\nadaptive=true: makes gamma adaptively adjust during the iterations; this is by default gamma === nothing.\nminimum_gamma=1e-7: lower bound to gamma in case adaptive == true.\nextrapolation_sequence=nothing: sequence (iterator) of extrapolation coefficients to use for acceleration.\n\nReferences\n\nTseng, \"On Accelerated Proximal Gradient Methods for Convex-Concave Optimization\" (2008).\nBeck, Teboulle, \"A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems\", SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183-202 (2009).\n\n\n\n\n\n","category":"type"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.PANOC","page":"Problem types and algorithms","title":"ProximalAlgorithms.PANOC","text":"PANOC(; <keyword-arguments>)\n\nConstructs the PANOC algorithm [1].\n\nThis algorithm solves optimization problems of the form\n\nminimize f(Ax) + g(x),\n\nwhere f is smooth and A is a linear mapping (for example, a matrix).\n\nThe returned object has type IterativeAlgorithm{PANOCIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: PANOCIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=1_000: maximum number of iteration\ntol::1e-8: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=10: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the PANOCIteration constructor upon call\n\nReferences\n\nStella, Themelis, Sopasakis, Patrinos, \"A simple and efficient algorithm for nonlinear model predictive control\", 56th IEEE Conference on Decision and Control (2017).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.PANOCIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.PANOCIteration","text":"PANOCIteration(; <keyword-arguments>)\n\nIterator implementing the PANOC algorithm [1].\n\nThis iterator solves optimization problems of the form\n\nminimize f(Ax) + g(x),\n\nwhere f is smooth and A is a linear mapping (for example, a matrix).\n\nSee also: PANOC.\n\nArguments\n\nx0: initial point.\nf=Zero(): smooth objective term.\nA=I: linear operator (e.g. a matrix).\ng=Zero(): proximable objective term.\nLf=nothing: Lipschitz constant of the gradient of x ↦ f(Ax).\ngamma=nothing: stepsize to use, defaults to 1/Lf if not set (but Lf is).\nadaptive=false: forces the method stepsize to be adaptively adjusted.\nminimum_gamma=1e-7: lower bound to gamma in case adaptive == true.\nmax_backtracks=20: maximum number of line-search backtracks.\ndirections=LBFGS(5): strategy to use to compute line-search directions.\n\nReferences\n\nStella, Themelis, Sopasakis, Patrinos, \"A simple and efficient algorithm for nonlinear model predictive control\", 56th IEEE Conference on Decision and Control (2017).\n\n\n\n\n\n","category":"type"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.ZeroFPR","page":"Problem types and algorithms","title":"ProximalAlgorithms.ZeroFPR","text":"ZeroFPR(; <keyword-arguments>)\n\nConstructs the ZeroFPR algorithm [1].\n\nThis algorithm solves optimization problems of the form\n\nminimize f(Ax) + g(x),\n\nwhere f is smooth and A is a linear mapping (for example, a matrix).\n\nThe returned object has type IterativeAlgorithm{ZeroFPRIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: ZeroFPRIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=1_000: maximum number of iteration\ntol::1e-8: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=10: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the ZeroFPRIteration constructor upon call\n\nReferences\n\nThemelis, Stella, Patrinos, \"Forward-backward envelope for the sum of two nonconvex functions: Further properties and nonmonotone line-search algorithms\", SIAM Journal on Optimization, vol. 28, no. 3, pp. 2274-2303 (2018).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.ZeroFPRIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.ZeroFPRIteration","text":"ZeroFPRIteration(; <keyword-arguments>)\n\nIterator implementing the ZeroFPR algorithm [1].\n\nThis iterator solves optimization problems of the form\n\nminimize f(Ax) + g(x),\n\nwhere f is smooth and A is a linear mapping (for example, a matrix).\n\nSee also: ZeroFPR.\n\nArguments\n\nx0: initial point.\nf=Zero(): smooth objective term.\nA=I: linear operator (e.g. a matrix).\ng=Zero(): proximable objective term.\nLf=nothing: Lipschitz constant of the gradient of x ↦ f(Ax).\ngamma=nothing: stepsize to use, defaults to 1/Lf if not set (but Lf is).\nadaptive=false: forces the method stepsize to be adaptively adjusted.\nminimum_gamma=1e-7: lower bound to gamma in case adaptive == true.\nmax_backtracks=20: maximum number of line-search backtracks.\ndirections=LBFGS(5): strategy to use to compute line-search directions.\n\nReferences\n\nThemelis, Stella, Patrinos, \"Forward-backward envelope for the sum of two nonconvex functions: Further properties and nonmonotone line-search algorithms\", SIAM Journal on Optimization, vol. 28, no. 3, pp. 2274-2303 (2018).\n\n\n\n\n\n","category":"type"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.DRLS","page":"Problem types and algorithms","title":"ProximalAlgorithms.DRLS","text":"DRLS(; <keyword-arguments>)\n\nConstructs the Douglas-Rachford line-search algorithm [1].\n\nThis algorithm solves convex optimization problems of the form\n\nminimize f(x) + g(x),\n\nwhere f is smooth.\n\nThe returned object has type IterativeAlgorithm{DRLSIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: DRLSIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=1_000: maximum number of iteration\ntol::1e-8: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=10: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the DRLSIteration constructor upon call\n\nReferences\n\nThemelis, Stella, Patrinos, \"Douglas-Rachford splitting and ADMM for nonconvex optimization: Accelerated and Newton-type linesearch algorithms\", arXiv:2005.10230, 2020.\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.DRLSIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.DRLSIteration","text":"DRLSIteration(; <keyword-arguments>)\n\nIterator implementing the Douglas-Rachford line-search algorithm [1].\n\nThis iterator solves optimization problems of the form\n\nminimize f(x) + g(x),\n\nwhere f is smooth.\n\nSee also: DRLS.\n\nArguments\n\nx0: initial point.\nf=Zero(): smooth objective term.\ng=Zero(): proximable objective term.\nmf=nothing: convexity modulus of f.\nLf=nothing: Lipschitz constant of the gradient of f.\ngamma: stepsize to use, chosen appropriately based on Lf and mf by defaults.\nmax_backtracks=20: maximum number of line-search backtracks.\ndirections=LBFGS(5): strategy to use to compute line-search directions.\n\nReferences\n\nThemelis, Stella, Patrinos, \"Douglas-Rachford splitting and ADMM for nonconvex optimization: Accelerated and Newton-type linesearch algorithms\", arXiv:2005.10230, 2020.\n\n\n\n\n\n","category":"type"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.PANOCplus","page":"Problem types and algorithms","title":"ProximalAlgorithms.PANOCplus","text":"PANOCplus(; <keyword-arguments>)\n\nConstructs the the PANOCplus algorithm [1].\n\nThis algorithm solves optimization problems of the form\n\nminimize f(Ax) + g(x),\n\nwhere f is locally smooth and A is a linear mapping (for example, a matrix).\n\nThe returned object has type IterativeAlgorithm{PANOCplusIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: PANOCplusIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=1_000: maximum number of iteration\ntol::1e-8: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=10: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the PANOCplusIteration constructor upon call\n\nReferences\n\nDe Marchi, Themelis, \"Proximal gradient algorithms under local Lipschitz gradient continuity: a convergence and robustness analysis of PANOC\", arXiv:2112.13000 (2021).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.PANOCplusIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.PANOCplusIteration","text":"PANOCplusIteration(; <keyword-arguments>)\n\nIterator implementing the PANOCplus algorithm [1].\n\nThis iterator solves optimization problems of the form\n\nminimize f(Ax) + g(x),\n\nwhere f is locally smooth and A is a linear mapping (for example, a matrix).\n\nSee also: PANOCplus.\n\nArguments\n\nx0: initial point.\nf=Zero(): smooth objective term.\nA=I: linear operator (e.g. a matrix).\ng=Zero(): proximable objective term.\nLf=nothing: Lipschitz constant of the gradient of x ↦ f(Ax).\ngamma=nothing: stepsize to use, defaults to 1/Lf if not set (but Lf is).\nadaptive=false: forces the method stepsize to be adaptively adjusted.\nminimum_gamma=1e-7: lower bound to gamma in case adaptive == true.\nmax_backtracks=20: maximum number of line-search backtracks.\ndirections=LBFGS(5): strategy to use to compute line-search directions.\n\nReferences\n\nDe Marchi, Themelis, \"Proximal gradient algorithms under local Lipschitz gradient continuity: a convergence and robustness analysis of PANOC\", arXiv:2112.13000 (2021).\n\n\n\n\n\n","category":"type"},{"location":"guide/implemented_algorithms/#three_terms_splitting","page":"Problem types and algorithms","title":"Three-terms: f + g + h","text":"","category":"section"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"When more than one non-differentiable term is there in the objective, algorithms from the previous section do not in general apply out of the box, since operatornameprox_gamma (g + h) does not have a closed form unless in particular cases. Therefore, ad-hoc iteration schemes have been studied.","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"Algorithm Assumptions Oracle Implementation References\nDavis-Yin f convex and smooth, g h convex nabla f, operatornameprox_gamma g, operatornameprox_gamma h DavisYin Damek Davis, Wotao Yin (2017)","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"ProximalAlgorithms.DavisYin\nProximalAlgorithms.DavisYinIteration","category":"page"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.DavisYin","page":"Problem types and algorithms","title":"ProximalAlgorithms.DavisYin","text":"DavisYin(; <keyword-arguments>)\n\nConstructs the Davis-Yin splitting algorithm [1].\n\nThis algorithm solves convex optimization problems of the form\n\nminimize f(x) + g(x) + h(x),\n\nwhere f is smooth.\n\nThe returned object has type IterativeAlgorithm{DavisYinIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: DavisYinIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=10_000: maximum number of iteration\ntol::1e-8: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=100: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the DavisYinIteration constructor upon call\n\nReferences\n\nDavis, Yin. \"A Three-Operator Splitting Scheme and its Optimization Applications\", Set-Valued and Variational Analysis, vol. 25, no. 4, pp. 829–858 (2017).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.DavisYinIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.DavisYinIteration","text":"DavisYinIteration(; <keyword-arguments>)\n\nIterator implementing the Davis-Yin splitting algorithm [1].\n\nThis iterator solves convex optimization problems of the form\n\nminimize f(x) + g(x) + h(x),\n\nwhere f is smooth.\n\nSee also DavisYin.\n\nArguments\n\nx0: initial point.\nf=Zero(): smooth objective term.\ng=Zero(): proximable objective term.\nh=Zero(): proximable objective term.\nLf=nothing: Lipschitz constant of the gradient of h.\ngamma=nothing: stepsize to use, defaults to 1/Lf if not set (but Lf is).\n\nReferences\n\nDavis, Yin. \"A Three-Operator Splitting Scheme and its Optimization Applications\", Set-Valued and Variational Analysis, vol. 25, no. 4, pp. 829-858 (2017).\n\n\n\n\n\n","category":"type"},{"location":"guide/implemented_algorithms/#primal_dual_splitting","page":"Problem types and algorithms","title":"Primal-dual: f + g + h circ L","text":"","category":"section"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"When a function h is composed with a linear operator L, the proximal operator of h circ L does not have a closed form in general. For this reason, specific algorithms by the name of \"primal-dual\" splitting schemes are often applied to this model.","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"Algorithm Assumptions Oracle Implementation References\nChambolle-Pock fequiv 0, g h convex, L linear operator operatornameprox_gamma g, operatornameprox_gamma h, L, L^* ChambollePock Antonin Chambolle, Thomas Pock (2011)\nVu-Condat f convex and smooth, g h convex, L linear operator nabla f, operatornameprox_gamma g, operatornameprox_gamma h, L, L^* VuCodat Bằng Công Vũ (2013), Laurent Condat (2013)\nAFBA f convex and smooth, g h convex, L linear operator nabla f, operatornameprox_gamma g, operatornameprox_gamma h, L, L^* AFBA Puya Latafat, Panagiotis Patrinos (2017)","category":"page"},{"location":"guide/implemented_algorithms/","page":"Problem types and algorithms","title":"Problem types and algorithms","text":"ProximalAlgorithms.ChambollePock\nProximalAlgorithms.ChambollePockIteration\nProximalAlgorithms.VuCondat\nProximalAlgorithms.VuCondatIteration\nProximalAlgorithms.AFBA\nProximalAlgorithms.AFBAIteration","category":"page"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.ChambollePock","page":"Problem types and algorithms","title":"ProximalAlgorithms.ChambollePock","text":"ChambollePock(; <keyword-arguments>)\n\nConstructs the Chambolle-Pock primal-dual algorithm [1].\n\nThis algorithm solves convex optimization problems of the form\n\nminimize g(x) + h(L x),\n\nwhere g and h are possibly nonsmooth, and L is a linear mapping.\n\nThe returned object has type IterativeAlgorithm{AFBAIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: ChambollePockIteration, AFBAIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=10_000: maximum number of iteration\ntol::1e-5: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=100: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the AFBAIteration constructor upon call\n\nReferences\n\nChambolle, Pock, \"A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging\", Journal of Mathematical Imaging and Vision, vol. 40, no. 1, pp. 120-145 (2011).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.ChambollePockIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.ChambollePockIteration","text":"ChambollePockIteration(; <keyword-arguments>)\n\nIterator implementing the Chambolle-Pock primal-dual algorithm [1].\n\nThis iterator solves convex optimization problems of the form\n\nminimize g(x) + h(L x),\n\nwhere g and h are possibly nonsmooth, and L is a linear mapping.\n\nSee also: AFBAIteration, ChambollePock.\n\nThis iteration is equivalent to AFBAIteration with theta=2, f=Zero(), l=IndZero(); for all other arguments see AFBAIteration.\n\nReferences\n\nChambolle, Pock, \"A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging\", Journal of Mathematical Imaging and Vision, vol. 40, no. 1, pp. 120-145 (2011).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.VuCondat","page":"Problem types and algorithms","title":"ProximalAlgorithms.VuCondat","text":"VuCondat(; <keyword-arguments>)\n\nConstructs the Vũ-Condat primal-dual algorithm [1, 2].\n\nThis algorithm solves convex optimization problems of the form\n\nminimize f(x) + g(x) + (h □ l)(L x),\n\nwhere f is smooth, g and h are possibly nonsmooth and l is strongly convex. Symbol □ denotes the infimal convolution, and L is a linear mapping.\n\nThe returned object has type IterativeAlgorithm{AFBAIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: VuCondatIteration, AFBAIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=10_000: maximum number of iteration\ntol::1e-5: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=100: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the AFBAIteration constructor upon call\n\nReferences\n\nCondat, \"A primal-dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms\", Journal of Optimization Theory and Applications, vol. 158, no. 2, pp 460-479 (2013).\nVũ, \"A splitting algorithm for dual monotone inclusions involving cocoercive operators\", Advances in Computational Mathematics, vol. 38, no. 3, pp. 667-681 (2013).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.VuCondatIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.VuCondatIteration","text":"VuCondatIteration(; <keyword-arguments>)\n\nIterator implementing the Vũ-Condat primal-dual algorithm [1, 2].\n\nThis iterator solves convex optimization problems of the form\n\nminimize f(x) + g(x) + (h □ l)(L x),\n\nwhere f is smooth, g and h are possibly nonsmooth and l is strongly convex. Symbol □ denotes the infimal convolution, and L is a linear mapping.\n\nThis iteration is equivalent to AFBAIteration with theta=2; for all other arguments see AFBAIteration.\n\nSee also: AFBAIteration, VuCondat.\n\nReferences\n\nCondat, \"A primal-dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms\", Journal of Optimization Theory and Applications, vol. 158, no. 2, pp 460-479 (2013).\nVũ, \"A splitting algorithm for dual monotone inclusions involving cocoercive operators\", Advances in Computational Mathematics, vol. 38, no. 3, pp. 667-681 (2013).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.AFBA","page":"Problem types and algorithms","title":"ProximalAlgorithms.AFBA","text":"AFBA(; <keyword-arguments>)\n\nConstructs the asymmetric forward-backward-adjoint algorithm (AFBA, see [1]).\n\nThis algorithm solves convex optimization problems of the form\n\nminimize f(x) + g(x) + (h □ l)(L x),\n\nwhere f is smooth, g and h are possibly nonsmooth and l is strongly convex. Symbol □ denotes the infimal convolution, and L is a linear mapping.\n\nThe returned object has type IterativeAlgorithm{AFBAIteration}, and can be called with the problem's arguments to trigger its solution.\n\nSee also: AFBAIteration, IterativeAlgorithm.\n\nArguments\n\nmaxit::Int=10_000: maximum number of iteration\ntol::1e-5: tolerance for the default stopping criterion\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool=false: whether the algorithm state should be displayed\nfreq::Int=100: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: additional keyword arguments to pass on to the AFBAIteration constructor upon call\n\nReferences\n\nLatafat, Patrinos, \"Asymmetric forward-backward-adjoint splitting for solving monotone inclusions involving three operators\", Computational Optimization and Applications, vol. 68, no. 1, pp. 57-93 (2017).\nLatafat, Patrinos, \"Primal-dual proximal algorithms for structured convex optimization: a unifying framework\", In Large-Scale and Distributed Optimization, Giselsson and Rantzer, Eds. Springer International Publishing, pp. 97-120 (2018).\n\n\n\n\n\n","category":"function"},{"location":"guide/implemented_algorithms/#ProximalAlgorithms.AFBAIteration","page":"Problem types and algorithms","title":"ProximalAlgorithms.AFBAIteration","text":"AFBAIteration(; <keyword-arguments>)\n\nIterator implementing the asymmetric forward-backward-adjoint algorithm (AFBA, see [1]).\n\nThis iterator solves convex optimization problems of the form\n\nminimize f(x) + g(x) + (h □ l)(L x),\n\nwhere f is smooth, g and h are possibly nonsmooth and l is strongly convex. Symbol □ denotes the infimal convolution, and L is a linear mapping.\n\nPoints x0 and y0 are the initial primal and dual iterates, respectively. If unspecified, functions f, g, and h default to the identically zero function, l defaults to the indicator of the set {0}, and L defaults to the identity. Important keyword arguments, in case f and l are set, are the Lipschitz constants beta_f and beta_l (see below).\n\nThe iterator implements Algorithm 3 of [1] with constant stepsize (α_n=λ) for several prominant special cases:\n\nθ = 2          ==>   Corresponds to the Vu-Condat Algorithm [3, 4].\nθ = 1, μ=1\nθ = 0, μ=1\nθ ∈ [0,∞), μ=0\n\nSee [2, Section 5.2] and [1, Figure 1] for stepsize conditions, special cases, and relation to other algorithms.\n\nSee also: AFBA.\n\nArguments\n\nx0: initial primal point.\ny0: initial dual point.\nf=Zero(): smooth objective term.\ng=Zero(): proximable objective term.\nh=Zero(): proximable objective term.\nl=IndZero(): strongly convex function.\nL=I: linear operator (e.g. a matrix).\nbeta_f=0: Lipschitz constant of the gradient of f.\nbeta_l=0: Lipschitz constant of the gradient of l conjugate.\ntheta=1: nonnegative algorithm parameter.\nmu=1: algorithm parameter in the range [0,1].\ngamma1: primal stepsize (see [1] for the default choice).\ngamma2: dual stepsize (see [1] for the default choice).\n\nReferences\n\nLatafat, Patrinos, \"Asymmetric forward-backward-adjoint splitting for solving monotone inclusions involving three operators\", Computational Optimization and Applications, vol. 68, no. 1, pp. 57-93 (2017).\nLatafat, Patrinos, \"Primal-dual proximal algorithms for structured convex optimization: a unifying framework\", In Large-Scale and Distributed Optimization, Giselsson and Rantzer, Eds. Springer International Publishing, pp. 97-120 (2018).\nCondat, \"A primal-dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms\", Journal of Optimization Theory and Applications, vol. 158, no. 2, pp 460-479 (2013).\nVũ, \"A splitting algorithm for dual monotone inclusions involving cocoercive operators\", Advances in Computational Mathematics, vol. 38, no. 3, pp. 667-681 (2013).\n\n\n\n\n\n","category":"type"},{"location":"bibliography/#Bibliography","page":"Bibliography","title":"Bibliography","text":"","category":"section"},{"location":"bibliography/","page":"Bibliography","title":"Bibliography","text":"","category":"page"},{"location":"#ProximalAlgorithms.jl","page":"Home","title":"ProximalAlgorithms.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for non-smooth optimization algorithms. Link to GitHub repository.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package provides algorithms for the minimization of objective functions that include non-smooth terms, such as constraints or non-differentiable penalties. Implemented algorithms include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Fast) Proximal gradient methods\nDouglas-Rachford splitting\nThree-term splitting\nPrimal-dual splitting algorithms\nNewton-type methods","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check out this section for an overview of the available algorithms.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Install the latest stable release with","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add ProximalAlgorithms","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install the development version instead (master branch), do","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]\npkg> add ProximalAlgorithms#master","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use any of the algorithms from ProximalAlgorithms in your research, you are kindly asked to cite the relevant bibliography. Please check this section of the manual for algorithm-specific references.","category":"page"},{"location":"#Contributing","page":"Home","title":"Contributing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Contributions are welcome in the form of issue notifications or pull requests. When contributing new algorithms, we highly recommend looking at already implemented ones to get inspiration on how to structure the code.","category":"page"},{"location":"guide/custom_algorithms/#Custom-algorithms","page":"Custom algorithms","title":"Custom algorithms","text":"","category":"section"},{"location":"guide/custom_algorithms/","page":"Custom algorithms","title":"Custom algorithms","text":"warning: Warning\nThis page is under construction, and may be incomplete.","category":"page"},{"location":"guide/custom_algorithms/","page":"Custom algorithms","title":"Custom algorithms","text":"ProximalAlgorithms.IterativeAlgorithm","category":"page"},{"location":"guide/custom_algorithms/#ProximalAlgorithms.IterativeAlgorithm","page":"Custom algorithms","title":"ProximalAlgorithms.IterativeAlgorithm","text":"IterativeAlgorithm(T; maxit, stop, solution, verbose, freq, display, kwargs...)\n\nWrapper for an iterator type T, adding termination and verbosity options on top of it.\n\nThis is a conveniency constructor to allow for \"partial\" instantiation of an iterator of type T. The resulting \"algorithm\" object alg can be called on a set of keyword arguments, which will be merged to kwargs and passed on to T to construct an iterator which will be looped over. Specifically, if an algorithm is constructed as\n\nalg = IterativeAlgorithm(T; maxit, stop, solution, verbose, freq, display, kwargs...)\n\nthen calling it with\n\nalg(; more_kwargs...)\n\nwill internally loop over an iterator constructed as\n\nT(; alg.kwargs..., more_kwargs...)\n\nNote\n\nThis constructor is not meant to be used directly: instead, algorithm-specific constructors should be defined on top of it and exposed to the user, that set appropriate default functions for stop, solution, display.\n\nArguments\n\nT::Type: iterator type to use\nmaxit::Int: maximum number of iteration\nstop::Function: termination condition, stop(::T, state) should return true when to stop the iteration\nsolution::Function: solution mapping, solution(::T, state) should return the identified solution\nverbose::Bool: whether the algorithm state should be displayed\nfreq::Int: every how many iterations to display the algorithm state\ndisplay::Function: display function, display(::Int, ::T, state) should display a summary of the iteration state\nkwargs...: keyword arguments to pass on to T when constructing the iterator\n\n\n\n\n\n","category":"type"}]
}
